import math
import datasets
import joblib
from matplotlib import pyplot as plt, transforms
import pandas as pd
import torch
import torch.nn as nn
from tracker import ClientTracker, PunishedClientPool, make_client
from classifier_module import MetaClassifier, set_parameters
from nodes import FederatedClient, LazyClient, EchoClient, RandomClient, SmallClient
import numpy as np
from sklearn.metrics import f1_score
from sklearn.metrics import precision_recall_fscore_support
from flwr.common import parameters_to_ndarrays
import numpy as np
from flwr.common import ndarrays_to_parameters
from scipy.spatial.distance import cosine
import flwr as fl
from torch.utils.data import DataLoader, Subset

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class CustomFedAvgWithDetection(fl.server.strategy.FedAvg):
    def __init__(self, model, pool, tracker, penalty_threshold=3, penalty_mode: str = "accumulated",
                 switch_round=15, eval_dataset=None, decay=2):
        super().__init__()
        self.model = model
        self.pool = pool
        self.tracker = tracker
        self.penalty_threshold = penalty_threshold
        self.penalty_mode = penalty_mode
        self.switch_round = switch_round
        # self.input_dim = 268650
        self.input_dim = sum(p.numel() for p in model.parameters() if p.requires_grad)
        self.eval_dataset = eval_dataset  # ‚Üê Ïù¥ Ï§ÑÏù¥ Ï§ëÏöî!
        self.decay = decay

        self.window = 3
        self.a = 2.0
        self.b = 1.0

        self.loss_per_round = []      # Í∏ÄÎ°úÎ≤å validation loss per round
        self.normal_abnormal_count = []  # (normal_count, abnormal_count) per round

    # ‚úÖ F1 Í∏∞Î∞ò Ìå®ÎÑêÌã∞ Ï°∞Í±¥
    def is_f1_penalty(self, cid, threshold=0.7, window=3):
        trues, preds = self.tracker.get_recent_predictions(cid, num_rounds=window)
        if len(trues) < window:
            return False  # Îç∞Ïù¥ÌÑ∞ Î∂ÄÏ°± Ïãú Î≥¥Î•ò
        f1 = f1_score(trues, preds, average="macro", zero_division=0)
        print(f"[F1 Check] CID: {cid} - Macro F1: {f1:.4f}")
        return f1 < threshold

    def evaluate_global_model(self, parameters):
        model = self.model
        set_parameters(model, parameters)
        model.eval()

        test_loader = DataLoader(self.eval_dataset, batch_size=32)
        correct, total = 0, 0
        with torch.no_grad():
            for data, target in test_loader:
                output = model(data)
                pred = output.argmax(dim=1)
                correct += (pred == target).sum().item()
                total += target.size(0)

        return correct / total

    def should_remove_client_consecutive(self, cid: str) -> bool:
        return self.tracker.is_consecutively_penalized(cid, self.penalty_threshold)

    def should_remove_client_accumulated(self, cid: str) -> bool:
        return self.tracker.penalty_log.get(cid, []).count(True) >= self.penalty_threshold

    # ‚úÖ Ìå®ÎÑêÌã∞ Í∏∞Ï§Ä ÌåêÎã® Ìï®Ïàò
    def is_penalized(self, cid, threshold):
        history = self.tracker.penalty_log[cid]
        return sum(history) >= threshold
    
    def is_f1_below_threshold(self, cid, threshold):
        f1 = self.tracker.get_client_f1(cid)
        if f1 is None:
            return False
        return f1 < threshold

    def is_consecutive_f1_below_threshold(self, cid, threshold):
        history = self.tracker.get_client_f1_history(cid)
        if not history or len(history) < threshold:
            return False
        # ÏµúÍ∑º NÍ∞úÍ∞Ä Î™®Îëê f1 < threshold Ïù∏ÏßÄ ÌôïÏù∏
        return all(f1 < threshold for f1 in history[-threshold:])


    # ‚úÖ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï†úÍ±∞ Ïó¨Î∂Ä ÌåêÎã®
    def should_kick(self, cid, current_round, threshold, mode):
        if mode == "hybrid":
            if current_round < self.switch_round:
                return self.is_penalized(cid, threshold)
            else:
                return self.tracker.is_consecutively_penalized(cid, threshold)
        elif mode == "consecutive":
            return self.tracker.is_consecutively_penalized(cid, threshold)
        elif mode == "accumulated":
            return self.is_penalized(cid, threshold)
        else:
            raise ValueError(f"Unknown penalty mode: {mode}")

    def evaluate_global_loss(self, params):
        """Global modelÏùò validation lossÎ•º Í≥ÑÏÇ∞ (Ï†ïÏÉÅÏ†ÅÏù∏ ÌååÎùºÎØ∏ÌÑ∞ Ï†ÅÏö©)"""
        model = self.model
        model.load_state_dict(self.ndarrays_to_state_dict(params))  # ÏßÅÏ†ë state_dict Î≥ÄÌôò
        model.eval()

        val_loader = DataLoader(self.eval_dataset, batch_size=32, shuffle=False)
        criterion = nn.CrossEntropyLoss()

        total_loss = 0
        total_samples = 0

        with torch.no_grad():
            for inputs, targets in val_loader:
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                total_loss += loss.item() * inputs.size(0)
                total_samples += inputs.size(0)

        avg_loss = total_loss / total_samples
        return avg_loss

    def ndarrays_to_state_dict(self, ndarrays):
        """Flower parametersÎ•º SimpleCNNÏùò state_dictÎ°ú Î≥ÄÌôò"""
        model = self.model
        state_dict = model.state_dict()
        new_state_dict = {}

        # ndarraysÎäî Î¶¨Ïä§Ìä∏Î°ú Îì§Ïñ¥Ïò®Îã§ (list of np.ndarray)
        idx = 0
        for key in state_dict.keys():
            arr = ndarrays[idx]
            new_state_dict[key] = torch.tensor(arr)
            idx += 1
        return new_state_dict

    def aggregate_fit_internal(self, results, failures):
        """Aggregate parameters from clients."""
        if not results:
            return None, {}
        
        # (num_examples, [np.ndarray, np.ndarray, ...]) Î¶¨Ïä§Ìä∏
        weights_results = []
        for client_proxy, fit_res in results:
            ndarrays = parameters_to_ndarrays(fit_res.parameters)
            weights_results.append((fit_res.num_examples, ndarrays))
        
        # Í∞Å Î†àÏù¥Ïñ¥Î≥ÑÎ°ú Í∞ÄÏ§ëÌï©ÏùÑ Í≥ÑÏÇ∞
        num_layers = len(weights_results[0][1])
        total_examples = sum(num_examples for num_examples, _ in weights_results)

        # layerÎ≥ÑÎ°ú Í∞ÄÏ§ë ÌèâÍ∑†
        weighted_parameters = []
        for layer_idx in range(num_layers):
            layer_sum = np.zeros_like(weights_results[0][1][layer_idx])
            for num_examples, ndarrays in weights_results:
                layer_sum += num_examples * ndarrays[layer_idx]
            layer_avg = layer_sum / total_examples
            weighted_parameters.append(layer_avg)

        aggregated_parameters = ndarrays_to_parameters(weighted_parameters)
        aggregated_metrics = {}
        return aggregated_parameters, aggregated_metrics

    def get_linear_dynamic_thresholds(self, current_round, total_rounds=30, min_lower=0.0010, min_upper=0.0015, max_upper=0.35):
        """
        ÎùºÏö¥ÎìúÍ∞Ä ÏßÑÌñâÎê®Ïóê Îî∞Îùº penalty ÏûÑÍ≥ÑÍ∞íÏùÑ Ï†êÏßÑÏ†ÅÏúºÎ°ú Ï§ÑÏûÑ.
        - current_round: ÌòÑÏû¨ ÎùºÏö¥Îìú Î≤àÌò∏
        - total_rounds: Ï†ÑÏ≤¥ ÎùºÏö¥Îìú Ïàò
        """
        # Ïòà: ÏÑ†Ìòï Í∞êÏÜå
        decay_ratio = current_round / total_rounds
        dynamic_upper = max_upper - (max_upper - min_upper) * decay_ratio

        # lowerÎäî Í≥†Ï†ï ÌòπÏùÄ ÏÇ¥ÏßùÎßå Î≥ÄÌôî (optional)
        dynamic_lower = min_lower  # ÌïÑÏöîÏãú Ïó¨Í∏∞ÎèÑ Ï§ÑÏó¨ÎèÑ Îê®

        return dynamic_lower, dynamic_upper
    
    def get_dynamic_thresholds(
        self,
        current_round,
        total_rounds=30,
        min_min=0.0005,    # ÏµúÏÜå ÏµúÏÜåÍ∞í
        max_min=0.01,      # Ï¥àÍ∏∞ ÏµúÏÜå ÏûÑÍ≥ÑÍ∞í
        min_upper=0.0015,
        max_upper=0.35
    ):
        """
        ÏßÄÏàòÏ†Å Í∞êÏÜå Î∞©ÏãùÏùÑ Ï†ÅÏö©Ìï¥ ÏÉÅ¬∑Ìïò ÏûÑÍ≥ÑÍ∞íÏùÑ ÎèôÏ†ÅÏúºÎ°ú Ï°∞Ï†ï.
        self.decay: ÏßÄÏàò Í∞êÏá† Í∞ïÎèÑ (Í∞íÏù¥ ÌÅ¥ÏàòÎ°ù Ï¥àÎ∞òÏóê Í∏âÍ≤©Ìûà Í∞êÏÜå)
        """
        decay_ratio = current_round / total_rounds

        # upper threshold (Ï†ïÏÉÅ ÎÖ∏Îìú Ìè¨Ìï® Í∏∞Ï§Ä)
        dynamic_upper = min_upper + (max_upper - min_upper) * np.exp(-self.decay * decay_ratio)

        # lower threshold (ÎπÑÏ†ïÏÉÅ ÎÖ∏Îìú Í±∏Îü¨ÎÇº ÌïòÌïú)
        dynamic_lower = min_min + (max_min - min_min) * np.exp(-self.decay * decay_ratio)

        return dynamic_lower, dynamic_upper


    def aggregate_fit(self, rnd, results, failures):
        print(f"[INFO] Aggregating results for round {rnd}...")

        true_labels = []
        pred_labels = []

        # üîµ Global model ÌååÎùºÎØ∏ÌÑ∞ Î≤°ÌÑ∞ Ï§ÄÎπÑ
        aggregated_params_ndarrays = parameters_to_ndarrays(
            self.aggregate_fit_internal(results, failures)[0]
        )
        global_vector = np.concatenate([p.flatten() for p in aggregated_params_ndarrays])
        self.tracker.add_param_log("server", "server", global_vector, rnd)

        # üîµ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Î≥Ñ Ï≤òÎ¶¨
        for client_proxy, fit_res in results:
            cid = fit_res.metrics["cid"]
            ctype = fit_res.metrics.get("type", "unknown")  # ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÌÉÄÏûÖ Í∞ÄÏ†∏Ïò§Í∏∞
            client = self.pool.get_client_by_id(cid)
            if client is None:
                continue

            # ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Î≤°ÌÑ∞Ìôî
            client_params = parameters_to_ndarrays(fit_res.parameters)
            client_vector = np.concatenate([p.flatten() for p in client_params])

            self.tracker.add_param_log(cid, ctype, client_vector, rnd)
            
            # Î≤°ÌÑ∞ Í±∞Î¶¨ Í≥ÑÏÇ∞
            distance = cosine(global_vector, client_vector)
            self.tracker.update_distance(cid, distance)

            true_label = getattr(client, "true_label", "unknown")
            print(f"[DEBUG] CID: {cid}, True: {true_label}, Distance: {distance:.4f}")
            true_labels.append(1 if true_label != "normal" else 0)

            # Í±∞Î¶¨ Í∏∞Î∞ò Ìå®ÎÑêÌã∞ Î∂ÄÏó¨ (Î≤°ÌÑ∞ Í±∞Î¶¨Í∞Ä ÌäπÏ†ï Í∏∞Ï§Ä Ïù¥Ìïò/Ïù¥ÏÉÅÏùº Îïå ÌéòÎÑêÌã∞)
            penalty_lower, penalty_upper = self.get_dynamic_thresholds(rnd)
            # Í∏∞Ï°¥ distance Í≥ÑÏÇ∞ ÌõÑ
            penalized = self.tracker.record_distance_penalty(
                cid,
                distance,
                penalty_lower,
                penalty_upper
            )
            if penalized:
                print(f"[‚ö†Ô∏è] Client {cid} penalized based on distance: {distance:.4f}")

            # Ìå®ÎÑêÌã∞ Î∂ÄÏó¨ Î∞è ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÍµêÏ≤¥
            if self.should_kick(cid, rnd, self.penalty_threshold, self.penalty_mode):
                print(f"[‚ö†Ô∏è] Client {cid} penalized and kicked based on mode {self.penalty_mode}")
                self.tracker.log_kick(rnd, cid, self.penalty_mode, true_label)
                new_client = self.pool.replace_client(self.model, cid, rnd, self.switch_round)
                self.tracker.log_add(rnd, new_client.cid, new_client.get_type())
                self.tracker.remove_from_suspect_pool(cid)


        # ‚úÖ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Î∂ÑÌè¨ Î∞è ÏÉÅÌÉú Í∏∞Î°ù
        self.tracker.log_distribution(rnd, [
            {"type": self.pool.get_client_by_id(cid).true_label}
            for cid in self.pool.clients
        ])
        self.tracker.log_client_pool_status(rnd, self.pool)
        self.tracker.save_all_logs()

        # ‚úÖ Global FL Accuracy Í∏∞Î°ù
        last_params = parameters_to_ndarrays(results[-1][1].parameters)
        fl_acc = self.evaluate_global_model(last_params)
        self.tracker.log_fl_accuracy(rnd, fl_acc)

        # üîµ Global model loss Ï∏°Ï†ï
        aggregated_parameters, _ = self.aggregate_fit_internal(results, failures)
        ndarrays = parameters_to_ndarrays(aggregated_parameters)
        loss = self.evaluate_global_loss(ndarrays)
        self.tracker.log_global_loss(rnd, loss)

        # üîµ C-LSS Í∏∞Î°ù
        losses = self.tracker.get_global_loss_list()
        alpha = self.tracker.get_last_abnormal_ratio()
        beta = self.tracker.get_last_added_ratio()

        if rnd >= self.window:
            recent_losses = losses[rnd - self.window:rnd]
            avg_recent = np.mean(recent_losses)
            current_loss = losses[rnd - 1]
            gamma = (1 + self.a * alpha + self.b * beta) / np.log(rnd + 2)
            clss = (current_loss - avg_recent) / gamma
            self.tracker.log_clss(rnd, clss)
        
        if rnd % 5 == 0:
            self.tracker.visualize_client_vectors_by_round(rnd)
            self.tracker.visualize_client_vectors_3d()

        return super().aggregate_fit(rnd, results, failures)


def plot_fl_accuracy_comparison(trackers, labels):
    plt.figure(figsize=(12, 6))
    for tracker, label in zip(trackers, labels):
        df = pd.DataFrame(tracker.fl_accuracies)  # <- Ï†ïÌôïÌïú ÏÜçÏÑ±Î™Ö ÏÇ¨Ïö©!
        if not df.empty and "fl_accuracy" in df.columns:
            plt.plot(df["round"], df["fl_accuracy"], label=label)
        else:
            print(f"[WARN] No 'fl_accuracy' data in tracker: {label}")
    plt.title("Global Model Accuracy Over Rounds")
    plt.xlabel("Round")
    plt.ylabel("FL Accuracy")
    plt.legend()
    plt.grid(True)
    plt.show()

